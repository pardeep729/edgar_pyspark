{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDGAR project - PySpark edition "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains all of the code and Windows command prompt/Linux terminal for deploying this project on a Hadoop server. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**N.B.** `Please read **ALL CELLS**`. There are commands to be run from the Windows command prompt or Linux terminal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The non-Python bit "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Connect to your server hosting Hadoop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am using an AWS EMR Linux server and used Windows command prompt and `ssh` to connect to it remotely."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From **Windows command prompt**: <br>\n",
    "> `ssh <pem file location> <username>@<server_address>` <br>\n",
    "\n",
    "e.g \n",
    "`ssh -i ~/.ssh/datalab_emr3.pem hadoop@ec2-12-345-678-900.eu-west-2.compute.amazonaws.com`      \n",
    "\n",
    "--- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Download raw files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To follow along, download the raw files from the GitHub directly into Linux storage using the following commands:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From **Linux terminal**: <br>\n",
    "> Make sure in home directory: <br>\n",
    "> `cd` <br><br>\n",
    "> For **10-k reports** of S&P 100 from last 12 years - download and unzip: <br>\n",
    "> `wget https://gitlab.com/pardeep7/edgar_public_v2/-/archive/master/edgar_public_v2-master.zip?path=10k_filings_raw -O 10k_raw_full.zip`<br>\n",
    ">> **N.B.** This is exactly what's in thr GitHub repo, but zipped up. I couldn't get it to upload to GitHub properly, hence why it says GitLab. Promise this is the only time. <br>\n",
    "\n",
    "> `mkdir raw_full` <br>\n",
    "> `unzip 10k_raw_full.zip -d raw_full` <br><br>\n",
    "> For Loughran-McDonald **sentiment word list**: <br>\n",
    "> `wget https://raw.githubusercontent.com/pardeep729/edgar_pyspark/main/sentiment_words.json -O sentiment_words.json`\n",
    "\n",
    "--- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Move raw files into  HDFS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting all of the files we just downloaded to linux storage into HDFS, so that Spark can use them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From **Linux terminal**: <br>\n",
    "> Make directories in HDFS: <br>\n",
    "> `hdfs dfs -mkdir /edgar` <br>\n",
    "> `hdfs dfs -mkdir /edgar/raw_full` <br><br>\n",
    "> Move files from within our linux directory into our HDFS directory <br>\n",
    "> `hdfs dfs -put raw_full/edgar_public_v2-master-10k_filings_raw/10k_filings_raw/* /edgar/raw_full` <br>\n",
    "> `hdfs dfs -put sentiment_words.json /edgar` <br>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Make directory in HDFS for results "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once our Spark program produces results, we will write them to this directory in HDFS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From **Linux terminal**: <br>\n",
    "> `hdfs dfs -mkdir /edgar/sentiment_counts` <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Install extra python libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our program needs **pandas** and **bs4**. If they are not installed, this will install them. If they are installed, this will tell you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From **Linux terminal**: <br>\n",
    "> `sudo python3 -m pip install pandas` <br>\n",
    "> `sudo python3 -m pip install beautifulsoup4` <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  The Python bit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  6. Run this .ipynb script from within PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have a couple options:\n",
    "1. Load this .ipynb file into **jupyter lab** installed on your Hadoop server. Then run from within there <br><br>\n",
    "2. Use **PySpark CLI** - copy and paste each **cell** into it individually and hit enter each time (**what I did**) <br><br>\n",
    "From **Linux terminal**: <br>\n",
    "> `pyspark` <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from time import time\n",
    "from datetime import timedelta\n",
    "\n",
    "from pyspark.sql.functions import input_file_name\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.functions import lower, col, split, explode, count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load raw data from HDFS into Spark DataFrames "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whatever data you're using, always get them into Spark DataFrames wherever possible. They are the most optimised thing to use in Spark, no matter how trivially small or excessively large your data is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Raw 10-k reports\n",
    "df = spark.read.text('/edgar/raw_full/*.html', wholetext=True).withColumn('raw file name', input_file_name())\n",
    "\n",
    "# Sentiment words - into a list of dfs (for joining later)\n",
    "sentiment_words = spark.read.json('/edgar/sentiment_words.json').toPandas().transpose().to_dict()[0]\n",
    "dfs = [spark.createDataFrame(data = [(i, ) for i in sentiment_words[k]], schema=[k]) for k in sentiment_words.keys()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup transformations on `df`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These essentially lay out the plan of action. But does **not** perform the actions, hence why they run relatively quickly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will register a user-defined function (`udf`) that encapsulates the cleaning logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python function\n",
    "def clean_line(raw_html_text):\n",
    "    '''\n",
    "    A function takes raw html text and outputs a clean version\n",
    "    '''\n",
    "    soup = BeautifulSoup(raw_html_text.strip(),\"html.parser\")\n",
    "    html_text = soup.text\n",
    "    cleaned_html_text = re.sub(r'[^a-zA-Z0-9 ]','', html_text)\n",
    "    return cleaned_html_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register as Spark UDF\n",
    "clean_line_UDF = udf(clean_line, StringType())\n",
    "spark.udf.register(\"clean_line_UDF\", clean_line, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply UDF to df, returning a \"cleaned\" df\n",
    "df_cleaned = df.withColumn('cleaned', clean_line_UDF(df['value'])).drop('value')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentiment extraction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using **joins** - a surprisingly efficient way of achieving this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split each document (row) into a list of words (for explosion later)\n",
    "df_with_split = df_cleaned.select('*', split(lower(col('cleaned')), ' ').alias('cleaned_split'))\n",
    "\n",
    "# Explode this table based on these lists (should get millions of rows)\n",
    "df_exploded = df_with_split.withColumn('exploded', explode('cleaned_split'))\n",
    "\n",
    "# Join each sentiment word list to the exploded dataframe\n",
    "dfs_joined = [ df_exploded.join(dfs[i], df_exploded['exploded'] == dfs[i][dfs[i].columns[0]], 'inner') for i in range(len(dfs))]\n",
    " \n",
    "# Count number of matches for each document and sentiment\n",
    "dfs_counts = [dfs_joined[i].groupBy('raw file name').agg(count('*').alias(dfs_joined[i].columns[-1])) for i in range(len(dfs_joined))]\n",
    " \n",
    "# Join all sentiment counts into one dataframe\n",
    "df_counts = df_with_split.select('raw file name')\n",
    "for sentiment in dfs_counts:\n",
    "    df_counts = df_counts.join(sentiment, 'raw file name', 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace all NULLS with 0\n",
    "df_counts = df_counts.na.fill(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup timer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can track how long the execution actually takes <br>\n",
    "***N.B.*** *You could use `%time`, but was found to not work sometimes. This method (below) always works.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class T():\n",
    "    def __enter__(self):\n",
    "        self.start = time()\n",
    "    def __exit__(self, type, value, traceback):\n",
    "        self.end = time()\n",
    "        elapsed = self.end - self.start\n",
    "        print(str(timedelta(seconds=elapsed)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Peform action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An action will actually run the execution plan, so is the beefy part of this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with T():\n",
    "    df_counts.write.option(\"header\",\"true\").option(\"sep\",\",\").mode(\"overwrite\").csv('/edgar/sentiment_counts/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Optional) Check results "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I like to import my results into a Spark DataFrame, just to eyeball them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = spark.read.option(\"header\",\"true\").csv('/edgar/sentiment_counts')\n",
    "test.show(20, truncate=False)\n",
    "test.count() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) non-Python bit - Getting results out of HDFS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's likely you want access your results file, so this is how you move stuff from HDFS to Linux storage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From **Linux terminal**: <br>\n",
    "> Merge files from HDFS results directory and output to HDFS: <br>\n",
    "> `hdfs dfs -cat /edgar/sentiment_counts/* | hdfs dfs -put - /edgar/sentiment_counts_merged/` <br><br>\n",
    ">> **N.B.** Likely our results are many files in the HDFS results directory (due to how Spark uses partitions). But we can combine them into a single file called `-` (default name, cannot change) <br>\n",
    "\n",
    "> Get file from HDFS into Linux (present working directory) <br>\n",
    "> `hdfs dfs -get /edgar/sentiment_counts_merged/- .` <br><br>\n",
    "> Rename it <br>\n",
    "> `cp - final.csv`<br><br>\n",
    "> Remove old file <br>\n",
    "> `rm -` <br>\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) non-Python bit - Getting results out of Linux"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming you're on a Windows machine, use `scp` to get the results file onto your local system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From **Windows command prompt**: <br>\n",
    "> ```scp <pem file location> <username>@<server_address>:/home/<username>/edgar/final.csv \"<Windows folder path>\" ```<br>\n",
    "\n",
    "e.g  \n",
    "    `ssh -i ~/.ssh/datalab_emr3.pem hadoop@ec2-12-345-678-900.eu-west-2.compute.amazonaws.com:/home/hadoop/edgar/final.csv .\\results\\`      \n",
    "\n",
    "\n",
    "--- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
